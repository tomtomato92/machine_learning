{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries to use for this exercise (please fun this cell first):\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "import scipy.io\n",
    "import matplotlib.cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the .mat data using scipy.io, which will be a dictionary datatype.\n",
    "data_dict = scipy.io.loadmat('ex4/ex4data1.mat')\n",
    "X=data_dict['X']\n",
    "y=data_dict['y']\n",
    "X=np.matrix(X)\n",
    "y=np.matrix(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imsave('filename.png', np.array(X[2000:2100,:]).reshape(200,200), cmap=cm.gray)\n",
    "print (plt.imshow(np.array(X[130,:]).reshape(20,20),cmap=cm.gray))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the weights\n",
    "weights = scipy.io.loadmat('ex4/ex4weights.mat')\n",
    "Theta1=weights['Theta1']\n",
    "Theta2=weights['Theta2']\n",
    "print (type(Theta1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a cost function to calculate cost of the neural networks\n",
    "def sigmoid(z):\n",
    "    g = np.zeros(np.shape(z))\n",
    "    g = 1/(1+np.exp(-z))\n",
    "    return g\n",
    "def nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambd):\n",
    "    #extract Theta1 and Theta2 from nn_params and note nn_params is an array type, we need Thetas to be matrix\n",
    "    Theta1 = nn_params[0:hidden_layer_size*(input_layer_size+1)]\n",
    "    Theta1 = Theta1.reshape(hidden_layer_size, input_layer_size+1)\n",
    "    Theta1 = np.matrix(Theta1)\n",
    "    Theta2 = nn_params[hidden_layer_size*(input_layer_size+1):]\n",
    "    Theta2 = Theta2.reshape(num_labels, hidden_layer_size + 1)\n",
    "    Theta2 = np.matrix(Theta2)\n",
    "    #initiaalize gradient for Thetas and they are arrays\n",
    "    Theta1_grad = np.zeros((np.shape(Theta1)))\n",
    "    Theta2_grad = np.zeros((np.shape(Theta2)))\n",
    "    #set up some useful variables\n",
    "    #Note X and y should be matrix after adding the bias still\n",
    "    [m, n] = np.shape(X)\n",
    "    additional_coulmn = np.ones((m,1))\n",
    "    X = np.append(additional_coulmn,X,axis=1)\n",
    "    #forward propagation calculation starting from input layer to hidden layer\n",
    "    Layer2 = X*Theta1.T\n",
    "    Layer2 = sigmoid(Layer2)\n",
    "    additional_coulmn = np.ones((m,1))\n",
    "    Layer2 = np.append(additional_coulmn,Layer2,axis=1)\n",
    "    #forward propagation calculation starting from hidden layer to output layer\n",
    "    Layer3 = Layer2*Theta2.T\n",
    "    predict = sigmoid(Layer3)\n",
    "    #transform y to be a matrix where each row's index corresponds to the correct label\n",
    "    ychange = np.zeros((m, num_labels))\n",
    "    for i in range(0,m):\n",
    "        row = ychange[i,:]\n",
    "        row[int(y[i])-1]=1\n",
    "        ychange[i,:]=row\n",
    "    ychange = np.matrix(ychange)\n",
    "    #calculate the cost    \n",
    "    J=(np.sum(np.multiply((-ychange),np.log(predict))-np.multiply((1-ychange),np.log(1-predict))))/m+(np.sum(np.multiply((Theta1[:,1:input_layer_size+1]),((Theta1[:,1:input_layer_size+1]))))+np.sum(np.multiply((Theta2[:,1:hidden_layer_size+1]),(Theta2[:,1:hidden_layer_size+1]))))*lambd/(2*m)\n",
    "\n",
    "    #(from matlab)J=(sum((-(log(predict)).*ychange-(log(1-predict)).*(-ychange+1)),'all'))/m+(sum(Theta1(:,[2:(size(Theta1,2))]).*Theta1(:,[2:(size(Theta1,2))]),'all')+sum(Theta2(:,[2:(size(Theta2,2))]).*Theta2(:,[2:(size(Theta2,2))]),'all'))*lambda/(2*m);\n",
    "    #(from logistic)J = np.sum(np.dot((-y.T),np.log(h))-np.dot((1-y).T,np.log(1-h)))/m + np.matrix(theta[1:])*np.transpose(np.matrix(theta[1:]))*lambd/(2*m)\n",
    "    \n",
    "    #back propagation: for each row (each prediction), calculate the grad for Theta1 and Theta2\n",
    "    for i in range (0,m):\n",
    "        #take out the first row of the input data\n",
    "        a1=X[i,:]\n",
    "        #calculate the hiddden layer\n",
    "        z2=a1*Theta1.T\n",
    "        a2=sigmoid(z2)\n",
    "        #add bias for calculating output layer\n",
    "        additional_coulmnfora2 = np.ones((np.shape(a2)[0],1))\n",
    "        a2 = np.append(additional_coulmnfora2,a2,axis=1)\n",
    "        z3=a2*Theta2.T\n",
    "        a3=sigmoid(z3)\n",
    "        #create a matrix where the row value == 1 indicates the prediction\n",
    "        yvec = np.zeros((1,num_labels))\n",
    "        yvec[0][int(y[i])-1]=1\n",
    "        yvec = np.matrix(yvec)\n",
    "        #back propagation\n",
    "        error3=a3-yvec\n",
    "        error2=np.multiply(Theta2.T*error3.T,(np.multiply(a2,(1-a2))).T)\n",
    "        Theta2_grad = Theta2_grad+error3.T*a2\n",
    "        Theta1_grad = Theta1_grad+error2[1:]*a1\n",
    "    #remove bias term's grad\n",
    "    Theta2passon = Theta2[:,1:hidden_layer_size+1]\n",
    "    additional_coulmnforTheta2passon = np.zeros((np.shape(Theta2passon)[0],1))\n",
    "    Theta2passon = np.append(additional_coulmnforTheta2passon,Theta2passon,axis=1)\n",
    "    Theta2_grad = Theta2_grad/m+(lambd/m)*Theta2passon\n",
    "    \n",
    "    Theta1passon = Theta1[:,1:input_layer_size+1]\n",
    "    additional_coulmnforTheta1passon = np.zeros((np.shape(Theta1passon)[0],1))\n",
    "    Theta1passon = np.append(additional_coulmnforTheta1passon,Theta1passon,axis=1)\n",
    "    Theta1_grad = Theta1_grad/m+(lambd/m)*Theta1passon\n",
    "    \n",
    "    grad = np.append(np.array(Theta1_grad), np.array(Theta2_grad))\n",
    "    \n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "\n",
    "nn_params = np.append(np.array(Theta1), np.array(Theta2))\n",
    "\n",
    "lambd = 1\n",
    "\n",
    "result = nnCostFunction(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, lambd)\n",
    "\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomly initialise weights\n",
    "def randInitializeWeights(L_in, L_out):\n",
    "    epsilon_init = 0.12;\n",
    "    W = np.random.rand(L_out, L_in+1) * 2 * epsilon_init - epsilon_init\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_Theta1 = randInitializeWeights(input_layer_size, hidden_layer_size);\n",
    "initial_Theta2 = randInitializeWeights(hidden_layer_size, num_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the optimal theta\n",
    "initial_nn_params = np.append(np.array(initial_Theta1), np.array(initial_Theta2))\n",
    "lambd =1\n",
    "input_layer_size  = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "result = opt.fmin_tnc(func=nnCostFunction, x0=np.array(initial_nn_params), args=(input_layer_size, hidden_layer_size, num_labels, X, y, lambd))\n",
    "print('Thetas found by fmin_tnc function: ', result)\n",
    "print('theta: \\n',result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=np.array(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_size  = 400\n",
    "hidden_layer_size = 25\n",
    "num_labels = 10\n",
    "\n",
    "lambd = 1\n",
    "\n",
    "result2 = nnCostFunction(nn, input_layer_size, hidden_layer_size, num_labels, X, y, lambd)\n",
    "\n",
    "print (result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
